{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR 2019:][iapr2019] Special project\n",
    "\n",
    "**Group members:**\n",
    "    1- Emma Nilsson,\n",
    "    2- Anna Fredriksson Häägg,\n",
    "    3- Herman Kristian Dieset\n",
    "\n",
    "**Due date:** 30.05.2019\n",
    "\n",
    "[iapr2019]: https://github.com/LTS5/iapr-2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Please find the description of this special project via [this link].\n",
    "\n",
    "[this link]: https://github.com/LTS5/iapr-2019/blob/master/project/special_project_description.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import scipy.io\n",
    "import skimage.io\n",
    "import skimage.color\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.color import rgb2gray\n",
    "import skimage.morphology as mp\n",
    "from skimage import measure\n",
    "import tensorflow as tf\n",
    "\n",
    "#import sys\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_base_path = os.path.join(os.pardir, 'data') # works for Herman\n",
    "data_base_path = ('data')\n",
    "data_folder = 'project-data'\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "#with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "#    tar.extractall(path=data_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create functions for loading images and annotations and make sure that each index match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images():\n",
    "\n",
    "    train_dir = os.path.join(data_base_path, data_folder +'/images/train/')\n",
    "    dir_train_list = sorted(os.listdir(train_dir))\n",
    "    train = [skimage.io.imread(os.path.join(train_dir, file)) for file in dir_train_list]\n",
    "\n",
    "    \n",
    "    test_dir = os.path.join(data_base_path, data_folder +'/images/test/')\n",
    "    dir_test_list = sorted(os.listdir(test_dir))\n",
    "    test = [skimage.io.imread(os.path.join(test_dir, file)) for file in dir_test_list]\n",
    "    \n",
    "    val_dir = os.path.join(data_base_path, data_folder +'/images/validation/')\n",
    "    dir_val_list = sorted(os.listdir(val_dir))\n",
    "    validation = [skimage.io.imread(os.path.join(val_dir, file)) for file in dir_val_list]\n",
    "    \n",
    "    return train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_file(filename):\n",
    "    \"\"\" Parse a PASCAL VOC xml file \"\"\"\n",
    "    tree = ET.parse(filename)\n",
    "    objects = []\n",
    "    for obj in tree.findall('object'):\n",
    "        obj_struct = {}\n",
    "        obj_struct['name'] = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        obj_struct['bbox'] = [int(float(bbox.find('xmin').text)),\n",
    "                              int(float(bbox.find('ymin').text)),\n",
    "                              int(float(bbox.find('xmax').text))-int(float(bbox.find('xmin').text)),\n",
    "                              int(float(bbox.find('ymax').text))-int(float(bbox.find('ymin').text))]\n",
    "        objects.append(obj_struct)\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations():\n",
    "    dir_train = os.path.join(data_base_path, data_folder + '/annotations/train/')\n",
    "    dir_train_list = sorted(os.listdir(dir_train))\n",
    "    train_files = [f for f in dir_train_list if os.path.isfile(os.path.join(dir_train, f))]\n",
    "    train_annotations = [parse_xml_file(os.path.join(dir_train, file)) for file in train_files]\n",
    "    \n",
    "    \n",
    "    dir_test = os.path.join(data_base_path, data_folder + '/annotations/test/')\n",
    "    dir_test_list = sorted(os.listdir(dir_test))\n",
    "    test_files = [f for f in dir_test_list if os.path.isfile(os.path.join(dir_test, f))]\n",
    "    test_annotations = [parse_xml_file(os.path.join(dir_test, file)) for file in test_files]\n",
    "    \n",
    "    dir_val = os.path.join(data_base_path, data_folder + '/annotations/validation/')\n",
    "    dir_val_list = sorted(os.listdir(dir_val))\n",
    "    val_files = [f for f in dir_val_list if os.path.isfile(os.path.join(dir_val, f))]\n",
    "    validation_annotations = [parse_xml_file(os.path.join(dir_val, file)) for file in val_files]\n",
    "    \n",
    "    return train_annotations, test_annotations, validation_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, validation = load_images()\n",
    "train_annotations, test_annotations, validation_annotations = load_annotations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_yellow(img):\n",
    "    \"\"\"\n",
    "    Remove yellow tones from the image.\n",
    "    \"\"\"\n",
    "    out = img.copy()\n",
    "    \n",
    "    # Get different channels\n",
    "    red = out[:,:,0]\n",
    "    green = out[:,:,1]\n",
    "    blue = out[:,:,2]\n",
    "    \n",
    "    # Define yellow and set all these parts to white in the image\n",
    "    is_yellow =  (red > 100) & (red < 250) & (green > 100) & (green < 250) & (blue > 0) & (blue < 200)\n",
    "    red[is_yellow] = 255\n",
    "    green[is_yellow] = 255\n",
    "    blue[is_yellow] = 255\n",
    "    \n",
    "    out[:,:,0] = red\n",
    "    out[:,:,1] = green\n",
    "    out[:,:,2] = blue\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(image, th1, th2):\n",
    "    \"\"\"Threshold an image between two thresholds.\"\"\"\n",
    "    th_img = image.copy()\n",
    "    th_img[th_img<th1] = 0\n",
    "    th_img[th_img>th2] = 0\n",
    "    th_img[(th_img>=th1) & (th_img<=th2)] = 255\n",
    "    return th_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erode(img, nb):\n",
    "    \"\"\"Apply erosion to image.\"\"\"\n",
    "    out = img.copy()\n",
    "    for i in range(nb):\n",
    "        out = mp.erosion(mp.erosion(skimage.img_as_ubyte(img)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef preprocess_image(img):\\n    img = remove_yellow(img)\\n    img_g = rgb2gray(img)\\n    img_th = mp.erosion(mp.erosion(threshold(skimage.img_as_ubyte(img_g), 1, 100)))\\n    return img_th\\n    '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_image(img):\n",
    "    \"\"\"Apply entire pre-processing pipeline to an image.\"\"\"\n",
    "    out = img.copy()\n",
    "    out = remove_yellow(out)\n",
    "    img_g = rgb2gray(out)\n",
    "    img_th = threshold(skimage.img_as_ubyte(img_g), 1, 100)\n",
    "    img_er = erode(img_th,1)\n",
    "    return img_er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(train, test, validation):\n",
    "    \"\"\"Pre-process all images.\"\"\"\n",
    "    train_out = train.copy()\n",
    "    test_out = test.copy()\n",
    "    val_out = validation.copy()\n",
    "    \n",
    "    for i in range(len(train_out)):\n",
    "        train_out[i] = preprocess_image(train_out[i])\n",
    "        \n",
    "    for i in range(len(test_out)):\n",
    "        test_out[i] = preprocess_image(test_out[i])\n",
    "        \n",
    "    for i in range(len(val_out)):\n",
    "        val_out[i] = preprocess_image(val_out[i])\n",
    "         \n",
    "    return train_out, test_out, val_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ada/lib/python3.6/site-packages/skimage/util/dtype.py:135: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    }
   ],
   "source": [
    "train_pro, test_pro, validation_pro = preprocess_images(train, test, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb train images     : 800\n",
      "Nb test images      : 50\n",
      "Nb validation images: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Nb train images     : {}\".format(len(train_pro)))\n",
    "print(\"Nb test images      : {}\".format(len(test_pro)))\n",
    "print(\"Nb validation images: {}\".format(len(validation_pro)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help functions for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp):\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(tp, fn):\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def f1_score(tp, fp, fn):\n",
    "    if (tp == 0):\n",
    "        return 0\n",
    "    return 2 * precision(tp, fp) * recall(tp, fn) / (precision(tp, fp) + recall(tp, fn))\n",
    "\n",
    "def iou_bbox(predict, true):\n",
    "    \"\"\"\n",
    "    Calculate the IoU for two bounding boxes.\n",
    "    Based on:\n",
    "    https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation\n",
    "    \"\"\"\n",
    "    if type(predict[0]) == list or type(true[0]) == list :\n",
    "        print((predict))\n",
    "        print(true)\n",
    "        #return 0\n",
    "    # Determine the coordinates of the intersection rectangle\n",
    "    x_left   = max(predict[0], true[0])\n",
    "    y_bottom = max(predict[1], true[1])\n",
    "    x_right  = min(predict[0] + predict[2], true[0] + true[2])\n",
    "    y_top    = min(predict[1] + predict[3], true[1] + true[3])\n",
    "\n",
    "    # If they do not overlap, return 0\n",
    "    if x_right < x_left or y_bottom > y_top:\n",
    "        return 0.0\n",
    "\n",
    "    intersection_area = (x_right - x_left) * (y_top - y_bottom)\n",
    "\n",
    "    # Compute area for both bounding boxes\n",
    "    bb1_area = predict[2] * predict[3]\n",
    "    bb2_area = true[2] * true[3]\n",
    "\n",
    "    # Compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the intersection area\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function which validates how a method works by setting IoU=0.3 and calculating precision, recall and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_method(method, test_between = None, printVals = False, test_imgs = test_pro):\n",
    "\n",
    "    def update_progress(progress):\n",
    "        \"\"\"Prints progress of the used validation method.\"\"\"\n",
    "        bar_length = 30\n",
    "        if isinstance(progress, int):\n",
    "            progress = float(progress)\n",
    "        if not isinstance(progress, float):\n",
    "            progress = 0\n",
    "        if progress < 0:\n",
    "            progress = 0\n",
    "        if progress >= 1:\n",
    "            progress = 1\n",
    "            \n",
    "        block = int(round(bar_length * progress))\n",
    "        clear_output(wait = True)\n",
    "        text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "        print(text)\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    # Setup progress bar\n",
    "    limit = len(test_imgs[test_between[0]:test_between[1]] if test_between != None else test_imgs)\n",
    "\n",
    "    for i in range(limit):\n",
    "        update_progress(i/limit)\n",
    "        # Predict varroas\n",
    "        varroas_pred = method(test_imgs[i])\n",
    "\n",
    "        # Get true bounding boxes\n",
    "        varroas_true = []\n",
    "        for bbox in test_annotations[i]:\n",
    "               varroas_true.append(bbox['bbox'])\n",
    "\n",
    "        # Get true and false positives\n",
    "        positives = []\n",
    "\n",
    "        for pb in varroas_pred:\n",
    "            true_pos = False\n",
    "\n",
    "            for tb in varroas_true:\n",
    "                if(iou_bbox(pb, tb) > 0.3):\n",
    "                    true_pos = True\n",
    "                    break;\n",
    "            positives.append(true_pos)\n",
    "\n",
    "        tp += positives.count(True)\n",
    "        fp += positives.count(False)\n",
    "\n",
    "        # Get the false negatives\n",
    "        negatives = []\n",
    "\n",
    "        for tb in varroas_true:\n",
    "            fn = False\n",
    "\n",
    "            for pb in varroas_pred:\n",
    "                if(iou_bbox(pb, tb) > 0.3):\n",
    "                    fn = True\n",
    "                    break;\n",
    "            negatives.append(fn)\n",
    "\n",
    "        # Count all times no intersection was found\n",
    "        fn += negatives.count(False)\n",
    "\n",
    "    update_progress(1)\n",
    "    \n",
    "    if printVals :\n",
    "        print('The values for the given test images')\n",
    "        print(tp)\n",
    "        print(fn)\n",
    "        print(fp)\n",
    "\n",
    "    \n",
    "    print(\"\\n----------- SCORES -----------\")\n",
    "    print(\"Precision: {}\".format(precision(tp,fp)))\n",
    "    print(\"Recall   : {}\".format(recall(tp,fn)))\n",
    "    print(\"F1-score : {}\".format(f1_score(tp,fp,fn)))\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Finding varroas by segmentation\n",
    "Add your implementation for ''**detect_by_segmentation**'' function. Please make sure the input and output follows the mentioned format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contours(img):\n",
    "    contours = measure.find_contours(img, 0)\n",
    "    lengths = []\n",
    "    for cnt in contours:\n",
    "        lengths.append(len(cnt))\n",
    "    return contours, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_threshold_length(contours, th_low = 0, th_high = np.Inf) :\n",
    "    # Filters list of contours, keeps those within threshold limits\n",
    "    out = []\n",
    "    for n, contour in enumerate(contours):\n",
    "        if len(contour) > th_low and len(contour) < th_high :\n",
    "            out.append(contour)\n",
    "    return out, len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contours2boxlist(contours):\n",
    "    boxes = []\n",
    "    for contour in contours :\n",
    "        x_min = int(np.min(contour[:,1]))\n",
    "        x_max = int(np.max(contour[:,1]))\n",
    "        y_min = int(np.min(contour[:,0]))\n",
    "        y_max = int(np.max(contour[:,0]))\n",
    "        \n",
    "        boxes.append([x_min, y_min, x_max - x_min, y_max - y_min])\n",
    "    \n",
    "    return boxes # [[x_1, y_1, w_1, h_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_by_segmentation(img):\n",
    "    '''\n",
    "    Input: One single image\n",
    "    Output: A numpy array containing coordonates of all detected varroas, with the following format: \n",
    "            [[x_1, y_1, w_1, h_2], [x_2, y_2, w_1, h_2], ..., [x_n, y_n, w_n, h_n]] \n",
    "            where ''n'' is the number of detected varroas.\n",
    "    '''\n",
    "    #img = preprocess_image(img)\n",
    "    contours,_ = find_contours(img)\n",
    "    contours,_ = contour_threshold_length(contours,40,80)\n",
    "    boxlist = contours2boxlist(contours)\n",
    "    \n",
    "    return boxlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your implementation. Report the Precision, Recall and F1-score, by using all 50 images of the test-set, and considering 0.3 as the IoU threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [##############################] 100.0%\n",
      "\n",
      "----------- SCORES -----------\n",
      "Precision: 0.04203670811130847\n",
      "Recall   : 0.9861111111111112\n",
      "F1-score : 0.08063600227143669\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Your code\n",
    "validate_method(detect_by_segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement your first detector\n",
    "\n",
    "Write your function(s) for the second part. Feel free to change the name of the function and add your additional functions, but please make sure their input and output follows the mentioned format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a sliding window function to be able to slide over the image and detect objects at different parts of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, stepSize, windowSize):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0], stepSize):\n",
    "        for x in range(0, image.shape[1], stepSize):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create our training features for the classifier. This is done by first finding the contours for all training annotations and then using them to compute the first 6 fourier descriptors of all contours. These will be our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contours(images, annotations):\n",
    "    '''\n",
    "    Input: Training images and training annotations\n",
    "    Output: Max length of the contours and an array with the contours of all training varroas\n",
    "    '''\n",
    "    max_len = 0\n",
    "    all_contours = []\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        for anno in annotations[i]:\n",
    "            bbox = anno['bbox']\n",
    "            img_box = images[i][bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]].copy()\n",
    "            im_bw = img_box\n",
    "\n",
    "            # Find contours using opencv\n",
    "            _, contours, _ = cv2.findContours(im_bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "            # Reshape to easily get x and y\n",
    "            if (len(contours) != 0):\n",
    "                contours_np = contours[0][:, 0, :]\n",
    "                all_contours.append(contours_np)\n",
    "                \n",
    "                # See if max length of contours has changed\n",
    "                if (max_len < contours_np.shape[0]):\n",
    "                    max_len = contours_np.shape[0]\n",
    "\n",
    "    return max_len, np.array(all_contours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our contours have the same length, so we need to pad them to calculate the fourier descriptors. Since we pad with 0, this might cause some errors to our data but we figure it works well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_descriptor(contours, length):\n",
    "    '''\n",
    "    Input: one contour, max length of all contours\n",
    "    Output: The 10 first fourrier descriptors of the contour\n",
    "    '''\n",
    "    \n",
    "    # Pad contour to max contour length\n",
    "    for i in range(length - len(contours)):\n",
    "        contours = np.concatenate([contours, np.array([[0, 0]])])\n",
    "    \n",
    "    # Create imag nums and descriptors\n",
    "    u_k = contours[:, 0] + 1j*contours[:, 1]\n",
    "    \n",
    "    fr = np.fft.fft(u_k)\n",
    "    \n",
    "    # Only return the 2nd and 3rd\n",
    "    return [fr[1], fr[2], fr[3], fr[4], fr[5], fr[6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and normalize training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len, contours = get_contours(train_pro, train_annotations)\n",
    "features = np.abs([(fourier_descriptor(c, max_len)) for c in contours])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.c_[ features, np.zeros(features.shape[0]) ]  \n",
    "scaler = preprocessing.StandardScaler().fit(features[:, 0:6])\n",
    "features_s = scaler.transform(features[:, 0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma='auto_deprecated',\n",
       "      kernel='rbf', max_iter=-1, nu=0.95, random_state=None,\n",
       "      shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_one = svm.OneClassSVM(nu=0.9)\n",
    "clf_svm_one.fit(features_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not miss any varroas the sliding window will take quite small steps (6-8px). This means that one varroa might be found several times. A merge_box function is implemented to merge overlapping bounding boxes before returning them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_boxes(bboxes_in):\n",
    "    bboxes = bboxes_in.copy()\n",
    "    for i in range(len(bboxes)):\n",
    "        for j in range(i+1, len(bboxes)):\n",
    "            if(iou_bbox(bboxes[i], bboxes[j]) > 0.3):\n",
    "                print('eee')\n",
    "\n",
    "                # Determine the coordinates of the intersection rectangle\n",
    "                x_left = min(bboxes[i][0], bboxes[j][0])\n",
    "                y_top = min(bboxes[i][1], bboxes[j][1])\n",
    "                x_right = max(bboxes[i][0] + bboxes[i][2], bboxes[j][0] + bboxes[j][2])\n",
    "                y_bottom = max(bboxes[i][1] + bboxes[i][3], bboxes[j][1] + bboxes[j][3])\n",
    "                \n",
    "                bboxes[j] = [x_left, y_top, x_right - x_left, y_bottom - y_top]\n",
    "                del bboxes[i]\n",
    "                break\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_by_method_1(image):\n",
    "    '''\n",
    "    Input: One single image\n",
    "    Output: A numpy array containing coordonates of all detected varroas, with the following format: \n",
    "            [[x_1, y_1, w_1, h_2], [x_2, y_2, w_1, h_2], ..., [x_n, y_n, w_n, h_n]] \n",
    "            where ''n'' is the number of detected varroas.\n",
    "    '''\n",
    "    \n",
    "    window_shape = (45, 45)\n",
    "    padding = max_len\n",
    "    varroas = []\n",
    "    for window in sliding_window(image, 6, window_shape):\n",
    "        im_bw = window[2]\n",
    "\n",
    "        # Find contours using opencv\n",
    "        _, contours, _ = cv2.findContours(im_bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        #print(len(contours))\n",
    "\n",
    "        # Reshape to easily get x and y\n",
    "        if (len(contours) > 0):\n",
    "            contours_np = contours[0][:, 0, :]\n",
    "            descriptor = np.abs(fourier_descriptor(contours_np, padding))\n",
    "            test_ = scaler.transform(np.array([descriptor[:]]))\n",
    "            prob = clf_svm_one.predict(test_.reshape(1, -1))\n",
    "            # print(prob)\n",
    "            if (prob > 0):\n",
    "                varroas.append([window[0], window[1], window_shape[0], window_shape[1]])\n",
    "\n",
    "    return merge_boxes(varroas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation of result by repporting the Precision, Recall and F1-score, by using all 50 images of the test-set, and considering 0.3 as the IoU threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [##############################] 100.0%\n",
      "\n",
      "----------- SCORES -----------\n",
      "Precision: 0.09669211195928754\n",
      "Recall   : 0.9047619047619048\n",
      "F1-score : 0.17471264367816094\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "validate_method(detect_by_method_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our high recall but really low precision score, we can conclude that we catch most varroas, but also a lot of objects which are not varroas. Improved preprocessing for the images might have helped us elevate the precision, but we think deep learning will be the best solution to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using MLP and CNNs\n",
    "\n",
    "Add your implementation for the thrid part. Feel free to add your desirable functions, but please make sure you have proper functions for the final detection, where their input and output follows the same format as the previous parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we create training data by using the parts of images we know contain varroas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_varroa_images(images, annotations, width, height, channels=3):\n",
    "    '''\n",
    "    Input: Training images and training annotations\n",
    "    Output: An array with all varroa images\n",
    "    '''\n",
    "    varroas = []\n",
    "    for i in range(len(images)):\n",
    "        for anno in annotations[i]:\n",
    "            bbox = anno['bbox']\n",
    "            img_box = images[i][bbox[1]:bbox[1]+height, bbox[0]:bbox[0]+width].copy()\n",
    "\n",
    "            if (channels != 3):\n",
    "                img_box = np.expand_dims(img_box, axis=2)\n",
    "\n",
    "            if (img_box.shape == (height, width, channels)):\n",
    "                varroas.append(img_box)\n",
    "\n",
    "    return varroas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the classifier we need a negative class that represent images without varroas. To create a dataset for this we find the images without varroas, slide over them, and save some windows to our negative class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_second_class_images(images, annotations, width, height, channels=3):\n",
    "    # Find all images without varroas\n",
    "    empty_anno_index = []\n",
    "    for i in range(len(annotations)):\n",
    "        if len(annotations[i]) == 0:\n",
    "            empty_anno_index.append(i)\n",
    "\n",
    "    # Store the first 5 images (change later)\n",
    "    #print(len(empty_anno_index))\n",
    "    #empty_anno_index = empty_anno_index[:100]\n",
    "    empty_pics = [images[i] for i in empty_anno_index]\n",
    "    \n",
    "    no_varroas = []\n",
    "    \n",
    "    # Slide over images and append all windows to the second class data array\n",
    "    for image in empty_pics:\n",
    "        for window in sliding_window(image, 300, (height, width)):\n",
    "            no_varroa_img = window[2]\n",
    "            if (channels != 3):\n",
    "                no_varroa_img = np.expand_dims(no_varroa_img, axis=2)\n",
    "            if (no_varroa_img.shape == (height, width, channels)):\n",
    "                no_varroas.append(no_varroa_img)\n",
    "\n",
    "    return no_varroas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train features and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_shape = (45, 45)\n",
    "varroas_pos = np.stack(get_varroa_images(train, train_annotations, window_shape[0], window_shape[1]), axis=0)\n",
    "varroas_neg = np.stack(create_second_class_images(train, train_annotations, window_shape[0], window_shape[1]), axis=0)\n",
    "train_features = np.vstack((varroas_pos, varroas_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17450, 45, 45, 3)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pos = np.ones((varroas_pos.shape[0]))\n",
    "label_neg = np.zeros((varroas_neg.shape[0]))\n",
    "train_labels = np.concatenate((label_pos, label_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_float = train_features.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras imports, helpers etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomize the data we use when training. Since we have already created the features for the train dataset but not for validation, we split the data so we can use part of the train dataset as validation set. To be able to use more data, this could be improved by insted using the validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio=0.8, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    #np.random.seed(seed)\n",
    "    \n",
    "    # generate random indices\n",
    "    num_row = len(y)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    index_split = int(np.floor(ratio * num_row))\n",
    "    index_tr = indices[: index_split]\n",
    "    index_te = indices[index_split:]\n",
    "    \n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_te = x[index_te]\n",
    "    y_tr = y[index_tr]\n",
    "    y_te = y[index_te]\n",
    "    return x_tr, x_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping data for MLP\n",
    "train_linear = train_features_float.reshape((-1, 45*45*3)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_data(train_linear/255, train_labels)\n",
    "y_train = keras.utils.to_categorical(y_train, 2)\n",
    "y_test = keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13960, 6075)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp = Sequential()\n",
    "\n",
    "model_mlp.add(Dense(8192, activation='relu', input_dim=x_train.shape[1]))\n",
    "model_mlp.add(Dense(4096, activation='relu'))\n",
    "model_mlp.add(Dense(2048, activation='relu'))\n",
    "model_mlp.add(Dense(1024, activation='relu'))\n",
    "model_mlp.add(Dense(512, activation='relu'))\n",
    "\n",
    "model_mlp.add(Dense(2, activation='softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model_mlp.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10042 samples, validate on 2511 samples\n",
      "Epoch 1/5\n",
      "10042/10042 [==============================] - 91s 9ms/step - loss: 0.6817 - acc: 0.5850 - val_loss: 0.6774 - val_acc: 0.5806\n",
      "Epoch 2/5\n",
      "10042/10042 [==============================] - 103s 10ms/step - loss: 0.6748 - acc: 0.5856 - val_loss: 0.6754 - val_acc: 0.5806\n",
      "Epoch 3/5\n",
      "10042/10042 [==============================] - 94s 9ms/step - loss: 0.6734 - acc: 0.5856 - val_loss: 0.6745 - val_acc: 0.5806\n",
      "Epoch 4/5\n",
      "10042/10042 [==============================] - 89s 9ms/step - loss: 0.6725 - acc: 0.5856 - val_loss: 0.6737 - val_acc: 0.5806\n",
      "Epoch 5/5\n",
      "10042/10042 [==============================] - 94s 9ms/step - loss: 0.6717 - acc: 0.5856 - val_loss: 0.6729 - val_acc: 0.5806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e6b498c18>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=100,\n",
    "        epochs=5,\n",
    "        validation_data=(x_test, y_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a CNN with 3 convolutional layers. All layers use the kernel size 5x5, and max_pooling has the pool_size=(2, 2). We use one linear layer after the last max pooling layer before we calculate the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input layer\n",
    "input_ = Input(shape=(45, 45, 3), dtype='float32')\n",
    "\n",
    "# Create convolution and max pooling layers\n",
    "conv1 = Conv2D(filters=16, kernel_size=5, padding='valid', activation='relu', strides=1)(input_)\n",
    "max1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "conv2 = Conv2D(filters=32, kernel_size=5, padding='valid', activation='relu', strides=1)(max1)\n",
    "max2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "conv3 = Conv2D(filters=64, kernel_size=5, padding='valid', activation='relu', strides=1)(max2)\n",
    "max3 = MaxPooling2D(pool_size=(2,2))(conv3)\n",
    "\n",
    "# Flatten\n",
    "flatten = Flatten()(max3)\n",
    "\n",
    "# Dropout, activation\n",
    "linear = Dense(256, activation='relu')(flatten)\n",
    "output = Dense(2, activation='softmax')(linear)\n",
    "\n",
    "# Create and compile model\n",
    "# Both sgd and adam has been tested as optimizers, adam gave the best results\n",
    "adam = optimizers.Adam()\n",
    "model_cnn = Model(inputs=[input_], outputs=[output])\n",
    "model_cnn.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=adam,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the data by dividing it with 255 before sending it to be splitted in training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_data(train_features_float/255, train_labels)\n",
    "y_train = keras.utils.to_categorical(y_train, 2)\n",
    "y_test = keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we quickly achieve high validation accuracy, the model is trained with 5 epochs. The batch size is set to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13960 samples, validate on 3490 samples\n",
      "Epoch 1/5\n",
      "13960/13960 [==============================] - 20s 1ms/step - loss: 0.1309 - acc: 0.9575 - val_loss: 0.0546 - val_acc: 0.9834\n",
      "Epoch 2/5\n",
      "13960/13960 [==============================] - 21s 1ms/step - loss: 0.0590 - acc: 0.9822 - val_loss: 0.0425 - val_acc: 0.9848\n",
      "Epoch 3/5\n",
      "13960/13960 [==============================] - 21s 2ms/step - loss: 0.0525 - acc: 0.9830 - val_loss: 0.0411 - val_acc: 0.9857\n",
      "Epoch 4/5\n",
      "13960/13960 [==============================] - 21s 2ms/step - loss: 0.0480 - acc: 0.9840 - val_loss: 0.0360 - val_acc: 0.9894\n",
      "Epoch 5/5\n",
      "13960/13960 [==============================] - 21s 1ms/step - loss: 0.0429 - acc: 0.9862 - val_loss: 0.0359 - val_acc: 0.9911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ed8f2b1d0>"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=100,\n",
    "        epochs=5,\n",
    "        validation_data=(x_test, y_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_boxes_cv(bboxes_in):\n",
    "    rects, weights = cv2.groupRectangles(bboxes_in, groupThreshold=1, eps=1)\n",
    "    return rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_by_cnn(image):\n",
    "    '''\n",
    "    Input: One single image\n",
    "    Output: A numpy array containing coordonates of all detected varroas, with the following format: \n",
    "            [[x_1, y_1, w_1, h_2], [x_2, y_2, w_1, h_2], ..., [x_n, y_n, w_n, h_n]] \n",
    "            where ''n'' is the number of detected varroas.\n",
    "    '''\n",
    "    \n",
    "    window_shape = (45, 45)\n",
    "    varroas = []\n",
    "    for window in sliding_window((image)/255, 15, window_shape):\n",
    "        im_bw = window[2]\n",
    "\n",
    "        if (im_bw.shape == (window_shape[0], window_shape[1], 3)):\n",
    "            im_bw = np.reshape(im_bw, [1, 45, 45, 3])\n",
    "            prob = model_cnn.predict(im_bw)\n",
    "        \n",
    "            if (prob[0][1] > 0.95):\n",
    "                varroas.append([window[0], window[1], window_shape[0], window_shape[1]])\n",
    "\n",
    "    return merge_boxes_cv(varroas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [##############################] 100.0%\n",
      "\n",
      "----------- SCORES -----------\n",
      "Precision: 0.21448087431693988\n",
      "Recall   : 0.98125\n",
      "F1-score : 0.3520179372197309\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "validate_method(detect_by_cnn, test_imgs=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You can generate a json submission file by using the function ''**generate_pred_json**''. This prediction file can be uploaded online for evaluation (Please refer to section 3 of the project description for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def generate_pred_json(data, tag='baseline'):\n",
    "    '''\n",
    "    Input\n",
    "    - data: Is a dictionary d, such that:\n",
    "          d = { \n",
    "              \"ID_1\": [], \n",
    "              \"ID_2\": [[x_21, y_21, w_21, h_21], [x_22, y_22, w_22, h_22]], \n",
    "              ... \n",
    "              \"ID_i\": [[x_i1, y_i1, w_i1, h_i1], ..., [x_iJ, y_iJ, w_iJ, h_iJ]],\n",
    "              ... \n",
    "              \"ID_N\": [[x_N1, y_N1, w_N1, h_N1]],\n",
    "          }\n",
    "          where ID is the string id of the image (e.i. 5a05e86fa07d56baef59b1cb_32.00px_1) and the value the Kx4 \n",
    "          array of intergers for the K predicted bounding boxes (e.g. [[170, 120, 15, 15]])\n",
    "    - tag: (optional) string that will be added to the name of the json file.\n",
    "    Output\n",
    "      Create a json file, \"prediction_[tag].json\", conatining the prediction to EvalAI format.\n",
    "    '''\n",
    "    unvalid_key = []\n",
    "    _data = data.copy()\n",
    "    for key, value in _data.items():\n",
    "        try:\n",
    "            # Try to convert to numpy array and cast as closest int\n",
    "            print(key)\n",
    "            v = np.around(np.array(value)).astype(int)\n",
    "            # Check is it is a 2d array with 4 columns (x,y,w,h)\n",
    "            if v.ndim != 2 or v.shape[1] != 4:\n",
    "                unvalid_key.append(key)\n",
    "            # Id must be a string\n",
    "            if not isinstance(key, str):\n",
    "                unvalid_key.append(key)\n",
    "            _data[key] = v.tolist()\n",
    "        # Deal with not consistant array size and empty predictions\n",
    "        except (ValueError, TypeError):\n",
    "            unvalid_key.append(key)\n",
    "    # Remove unvalid key from dictionnary\n",
    "    for key in unvalid_key: del _data[key]\n",
    "    \n",
    "    with open('prediction_{}.json'.format(tag), 'w') as outfile:\n",
    "        json.dump(_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "    size = img.shape\n",
    "    if len(size) == 3:\n",
    "        ax.imshow(img)\n",
    "        ax.set_title('({} px, {} px, depth {})'.format(size[0], size[1], size[2]))\n",
    "    else:\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title('({} px, {} px), single channel'.format(size[0], size[1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_annotation(img_nb, annotations, img = None):\n",
    "    if annotations == [] : \n",
    "        print(\"No varroa here!\")\n",
    "        return None\n",
    "    fig,ax = plt.subplots(1,1,figsize=(6,6))\n",
    "    for anno in annotations:\n",
    "        rect = patches.Rectangle((anno['bbox'][0], \n",
    "                                  anno['bbox'][1]), \n",
    "                                  anno['bbox'][2],\n",
    "                                  anno['bbox'][3],\n",
    "            linewidth=1,edgecolor='r',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    if img is not None: # img from argument\n",
    "        size = img.shape\n",
    "        if len(size) == 3:\n",
    "            ax.imshow(img)\n",
    "            ax.set_title('img nb {}, no varroae {}, ({} px, {} px, depth {})'.format(\n",
    "                img_nb,len(annotations_xmls[img_nb]), size[0], size[1], size[2]))\n",
    "        else:\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.set_title('img nb {}, no varroae {}, ({} px, {} px), single channel'.format(\n",
    "                img_nb,len(annotations_xmls[img_nb]), size[0], size[1]))\n",
    "    else: # image from collection\n",
    "        size = img_collection[img_nb].shape\n",
    "        if len(size) == 3:\n",
    "            ax.imshow(img_collection[img_nb])\n",
    "            ax.set_title('img nb {}, no varroae {}, ({} px, {} px, depth {})'.format(\n",
    "                img_nb,len(annotations_xmls[img_nb]), size[0], size[1], size[2]))\n",
    "        else:\n",
    "            ax.imshow(img_collection[img_nb], cmap='gray')\n",
    "            ax.set_title('img nb {}, no varroae {}, ({} px, {} px), single channel'.format(\n",
    "                img_nb,len(annotations_xmls[img_nb]), size[0], size[1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(img, th_low = 0, th_high = np.Inf):\n",
    "    # Find contours\n",
    "    contours,l = find_contours(img)\n",
    "    plt.subplots(1,1,figsize=(6,6))\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    contours,l = contour_threshold_length(contours, th_low, th_high)\n",
    "    \n",
    "    for contour in contours:\n",
    "        plt.plot(contour[:, 1], contour[:, 0], linewidth=2)\n",
    "    \n",
    "    plt.title(\"{} contours between threholds {} and {}\".format(l,\n",
    "                                                              th_low,\n",
    "                                                              th_high))\n",
    "    plt.show()\n",
    "\n",
    "def plot_contours_histogram(img):\n",
    "    _, lengths = find_contours(img)\n",
    "    plt.subplots(1,1,figsize=(10,6))\n",
    "    plt.hist(lengths, bins='auto')\n",
    "    plt.title(\"Histogram with 'auto' bins\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxlist_and_annotations(img_nb, boxlist, annotations) :\n",
    "    fig,ax = plt.subplots(1,1,figsize=(6,6))\n",
    "    \n",
    "    # plot boxes\n",
    "    tmp_im = np.zeros(img_collection[img_nb].shape)\n",
    "    for box in boxlist:\n",
    "        #print(box)\n",
    "        for x in range(box[0], box[2] + box[0]) :\n",
    "            for y in range(box[1], box[3] + box[1]):\n",
    "                tmp_im[y,x] = 1.0\n",
    "    ax.imshow(tmp_im, cmap = 'gray')\n",
    "    \n",
    "    # plot annotations\n",
    "    if annotations != [] : \n",
    "        for anno in annotations:\n",
    "            rect = patches.Rectangle((anno['bbox'][0], \n",
    "                                      anno['bbox'][1]), \n",
    "                                      anno['bbox'][2],\n",
    "                                      anno['bbox'][3],\n",
    "                linewidth=1,edgecolor='r',facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "        no_varroa = len(annotations)\n",
    "    else:\n",
    "        no_varroa = 0\n",
    "\n",
    "    #ax.imshow(img_collection[img_nb], cmap='gray')\n",
    "    ax.set_title('img nb {}, no varroae {}, no boxes {}'.format(\n",
    "        img_nb,no_varroa, len(boxlist)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours_and_annotations(img_nb, contours, annotations) :\n",
    "    fig,ax = plt.subplots(1,1,figsize=(6,6))\n",
    "    \n",
    "    # plot contours\n",
    "    for contour in contours:\n",
    "        ax.plot(contour[:, 1], contour[:, 0], linewidth=2)\n",
    "    \n",
    "    # plot annotations\n",
    "    if annotations != [] :\n",
    "        for anno in annotations:\n",
    "            rect = patches.Rectangle((anno['bbox'][0], \n",
    "                                      anno['bbox'][1]), \n",
    "                                      anno['bbox'][2],\n",
    "                                      anno['bbox'][3],\n",
    "                linewidth=1,edgecolor='r',facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "        no_varroa = len(annotations)\n",
    "    else:\n",
    "        no_varroa = 0\n",
    "\n",
    "    #ax.imshow(img_collection[img_nb], cmap='gray')\n",
    "    ax.set_title('img nb {}, no varroae {}, no contours {}'.format(\n",
    "        img_nb,no_varroa, len(contours)))\n",
    "    \n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
